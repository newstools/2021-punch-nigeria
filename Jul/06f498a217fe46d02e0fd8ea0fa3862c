TikTok has said it will use automation to detect and remove many of the videos that violate its policies. According to engadget.com, for the past year, the service has been testing and tweaking systems to find and take down such content. It will roll out those systems in the US and Canada over the next few weeks. To start with, the algorithms will be on the lookout for posts that violate policies related to the safety of minors, violence, graphic content, nudity, sex, illegal activity and regulated goods. If the systems detect a violation, they’ll yank the video immediately and the user who posted it can appeal. Users can still flag videos for manual review as well. Automated reviews will be “reserved for content categories where our technology has the highest degree of accuracy,” TikTok said. Only one in 20 of the videos that have been automatically removed were false positives and should have remained on the platform, according to the company. TikTok hopes to improve the algorithms’ accuracy levels and notes that “requests to appeal a video’s removal have remained consistent.” TikTok says automation should free up its safety staff to focus on content that requires a more nuanced approach, including videos containing bullying, harassment, misinformation and hate speech. Crucially, the systems could reduce the number of potentially distressing videos that the safety team have to watch, such as those containing extreme violence or child exploitation. Facebook, for one, has been accused of not doing enough to protect the wellbeing and mental health of content moderators who are tasked with reviewing often-disturbing content. Elsewhere, TikTok is changing how it notifies users after they’re caught breaking rules. The platform now tracks the number, severity and frequency of violations. Users will see details about those in the account updates section of their inbox. They can also view information about the consequences of their actions, such as how long they’re suspended from posting or engaging with anyone else’s content.